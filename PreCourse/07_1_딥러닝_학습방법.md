- 소프트맥스 연산

  - 소프트맥스(softmax) 함수는 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산
  - 분류 문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 예측

- 신경망은 선형모델과 활성함수(activation function)를 합성한 함수

- 활성함수

  - 활성함수(activation function)는 R 위에 정의된 비선형(nonlinear)함수로서 딥러닝에서 매우 중요한 개념
  - 활성함수를 쓰지 않으면 딥러닝은 선형모형과 차이가 없음
  - 시그모이드(sigmoid) 함수나 tanh 함수는 전통적으로 많이 쓰이던 활성함수지만 딥러닝에선 ReLU 함수를 많이 씀

- 다층(multi-layer) 퍼셉트론(MLP)은 신경망이 여러층 합성된 함수
- 이론적으로는 2층 신경망으로도 임의의 연속함수를 근사할 수 있음
- 층이 깊을수록 목적함수를 근사하는데 필요한 뉴런(노드)의 숫자가 훨씬 빨리 줄어들어 좀 더 효율적으로 학습이 가능

- 역전파 알고리즘

  - 딥러닝은 역전파(backpropagatioin) 알고리즘을 이용하여 각 층에 사용된 패러미터를 학습
  - 각 층 패러미터의 그레디언트 벡터는 윗층부터 역순으로 계산
  - 합성함수 미분법인 연쇄법칙(chain-rule) 기반 자동미분(auto-differentiation)을 사용

- 1번

  - Overfitting

    - 과적합
    - 모델이 학습 데이터셋을 지나치게 학습하여 발생하는 문제
    - 학습 데이터셋에서는 성능이 높게 나타나지만, 성능 데이터셋에서 오히려 성능이 더 떨어짐
    - 해결법

      - 훈련데이터를 추가하거나 더 작은 모델을 활용
      - Dropout, Regularization Method, Early Stopping Rule 등의 방법론 활용

    - 훈련 데이터의 개수에 비해 사용하는 모델이 너무 큰 경우 발생
      - O
    - 일어났을 경우 Early Stop을 적용
      - O
    - 일어난다면 Dropout 기법을 적용
      - O

  - Underfitting

    - 과소적합
    - Overfitting의 반대 개념
    - 모델이 충분히 학습 데이터를 학습하지 않은 상태
    - 해결법

      - 학습을 더 오래 진행하거나 더 큰 모델을 활용

    - 일어났을 경우 학습을 더 오래 진행하거나 더 큰 모델을 활용
      - O
    - 일어났을 경우 모델에 reqularization method를 적용
      - X
      - Overfitting이 일어났을 경우 적용

- 2번

  - Gradient Descent
    - 경사하강법
    - 최적값을 찾기 위해 손실합수를 편미분한 기울기(Gradient)에 학습률을 곱한 값을 빼서 업데이트를 진행
    - 학습률(에타)는 해당 기울기가 얼만큼 이동할 지를 결정하는 하이퍼파라미터
    - Batch Gradient Descent
      - 배치 경사하강법
      - 전체 데이터셋을 바탕으로 진행
      - 학습 수렴 속도가 느림
    - Stochastic Gradient Descent
      - SGD
      - 확률적 경사하강법
      - 전체 데이터셋이 아닌 무작위로 추출한 샘플 데이터셋에 경사하강법을 진행하는 방법론
    - Mini-Batch Gradient Descent
      - 미니배치 경사하강법
      - 각 배치 안에서 미니배치를 활용하여 SGD를 진행하는 방법론
    - Momentum
      - 기울기에 관성 효과를 이용하여 더 빠르게 최적값에 도달할 수 있도록 하는 방법론
      - Gradient Desent에 가속도항을 추가
      - Local Minimum, Plateau 현상을 해결하는 방법론
    - Gradient 값 말고 학습률(Learning Rate)를 조절한 기법
      - 학습률이 너무 작다면 학습 시간이 너무 길어짐
      - 학습률이 너무 크면 값이 발산(exploding)
    - Agarad
      - 각 가중치마다 학습률을 Adaptive하게, 다르게 조절하여 각 가중치별 특성을 고려하여 학습을 효율적으로 돕는 방법론
      - 각 Gradient마다 이전 기울기의 누적 제곱합에 반비례하도록 학습률을 조정하여 업데이트를 진행
        - 이전에 큰 기울기로 업데이트가 진행된 가중치는 작게 업데이트
        - 이전에 작은 기울기로 업데이트가 진행된 가중치는 큰 비율로 업데이트
    - RMSProp
      - Adagrad를 더 발전시킨 방법론
      - 단순 누적 제곱합을 활용하는 것이 아니라 지수이동평균(Exponential Moving Average, EMA)을 활용하여 기울기를 업데이트
      - 가장 최근 time step 기울기를 많이 반영하고 과거의 기울기는 조금만 반영
    - Adam
      - Momentem과 Ada 계열 중 하나인 RMSProp의 장점만을 결합한 알고리즘

- 3번

  - 정규화 방법론
    - Overfitting을 방지
    - Dropout
      - 학습 과정 중 신경망의 뉴런 중 일부를 랜덤하게 부분적으로 생략하는 방법론
      - 학습이 진행될 때마다 뉴런을 무작위로 학습하여 매번 모델을 다르게 학습시칸다는 관점에서 앙상블 기법과 유사한 효과
    - Parameter Norm Penalty
      - L1 Norm
        - 오차의 절댓값 활용
      - L2 Norm
        - 오차 제곱합 활용
      - 손실함수에 해당 Norm Penalty Term을 추가적으로 활용하여 정규화를 진행
    - Batch Nomalization
      - 배치 정규화
      - 배치단위간에 데이터 분포의 차이가 발생할 수 있음
      - 학습 과정에서 각 배치별로 데이터의 평균과 분산을 이용해 정규화를 진행하는 방법론
      - 각 데이터 분포를 평균은 0, 표준편차는 1인 데이터의 분포로 조정
    - Noise Robustness
      - 레이어 중간에 노이즈를 추가하여 학습효과를 높이고 과적합을 방지하는 방법론
    - Early Stopping
      - 모델의 과적합을 방지하기 위해서 이전 에폭에 비해 검증 오차(Validation Loss)가 감소하다 증가하는 경우 학습을 종료시키는 방법론

- 4번

  - RNN
    - Vanilla RNN
      - 이전 Cell의 output 값이 현 시점의 input과 함께 연산이 되는 방법론
      - 시퀀스가 매우 길어지면 앞쪽 정보가 뒤에 있는 타임 스텝까지 충분히 전달되지 못하는 장기의존성(Long Term Dependency) 문제를 해결하지 못함
    - LSTM
      - Long Short Term Memory
      - 입력(input) 게이트, 망각(forget) 게이트, 출력(output) 게이트를 활용하여 장기의존성 문제를 해결하고자 함
      - 입력게이트
        - 새로운 정보 중 어떤 것을 저장할지 결정하는 게이트
      - 망각게이트
        - 과거의 정보 중 어떤 부분을 버릴지 말지 결정하는 게이트
      - 출력게이트
        - 입력된 데이터 중 어떤 정보를 출력으로 내보낼지 결정하는 게이트
    - GRU
      - LSTM의 변형구조
      - Update Gate, Reset Gate만 존재하며 Output Gate는 존재하지 않음
      - LSTM에 비해 파라미터 수가 더 작음

- 7번

  - 시그모이드(Sigmoid) 함수의 순전파, 역전파 과정

- 8번

  - 선형독립
    - Linearly Independent
    - v1,...vn의 벡터 중 하나인 vj가 이전 벡터들의 선형결합으로 표현되지 않는 경우를 의미
    - x1v1+x2v2\_...xnvn=0을 만족하는 해인 x1,...,xn이 모두 0인 경우를 의미
    - 선형방정식의 해는 유일
  - 선형종속
    - Linearly Dependent
    - 선형독립과 반대로 v1,...vn의 벡터 중 하나인 vj가 이전 벡터들의 선형결합으로 표현되는 경우를 의미
    - x1v1+x2v2\_...xnvn=0을 만족하는 해인 x1,...,xn 중 0이 아닌 xi가 존재하는 경우를 의미
    - 방정식의 해는 여러 개 존재

- 10번

  - 부분공간
    - Subspace
    - 선형 결합에 닫혀있는 벡터공간 R^n의 부분집합
    - Subspace 내 벡터들이 선형결합을 통해서 새로운 벡터를 만들어내도 그 벡터는 Subspace안의 또 다른 벡터라는 의미
  - 기저
    - Basis
    - 2가지 조건을 만족하는 부분공간 벡터들의 집합
      1. 해당 벡터들의 Span으로 부분공간 전체를 표현 가능(Fully Span)해야함
      2. 기저의 모든 재료 벡터들은 선형 독립이어야 함
    - 기저는 유일하지 않음
    - 기저를 구성하는 벡터의 개수는 유일하며 이를 차원이라고 정의함
  - 열 공간
    - Column Space
    - 행렬의 열 벡터(Column Vector)를 선형결합하여 얻어지는 벡터 공간을 의미
  - 계수
    - Rank
    - 해당 열 공간의 차원을 의미

- 4기

  - 4번

    - RNN은 hidden 노드가 방향을 가진 엣지로 연결되어 순환구조를 이루는 인공신경망의 한 종류
    - RNN은 input 길이에 상관없이 동일한 파리미터를 가지고 동작하는 모델
    - RNN은 input 길이가 길어질수록, 역전파시 그래디언트가 점차 줄어 학습능력이 떨어짐
    - RNN의 변현 종류로는 GRU, LSTM 등이 있음

  - 5번

    - 파라미터 수 계산
      - i=32, o=28, K=[6,1,5,5]
        - `(5*5+1)*6=156`
      - i=14, o=10, K=[16,6,5,5]
        - `(6*5*5+1)*16=2416`
      - i=5, o=1, K=[120,16,5,5]
        - `(16*5*5+1)*120=48120`
      - pooling layer는 0개
      - i=120, o=84
        - `84*(120+1)=10164`
      - i=84, o=10
        - `10*(84+1)=850`

- 3기

  - step의 수
    - 2050개의 데이터를 32번 볼 경우
    - `2050/32=64.0625`
    - 64회 반복하고 1step이 더 필요하기에 65번

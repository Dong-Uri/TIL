- Gradient Descent Methods

  - Stochastic gradient descent
    - Update with the gradient computed from a single sample
  - Mini-batch gradient descent
    - Update with the gradient computed from a subset of data
  - Batch gradient descent
    - Update with the gradient computed from the whole data
  - Batch-size Matters
  - (Stochastic) Gradient descent
  - Momentum
  - Nesterov Accelerated Gradient
  - Adagrad
    - Adagrad adapts the learning rate, performing larger updates for infrequent and smaller updates for frequent parameters
  - Adadelta
    - Adadelta extends Adagrad to reduce its monotonically decreasing the learning rate by restricting the accumulation window
  - RMSprop
    - RMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in his lecture
  - Adam
    - Adaptive Moment Estimation (Adam) leverages both past gradients and squared gradients
